{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "credit_card_fraud.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbVBb67CeNV5",
        "colab_type": "text"
      },
      "source": [
        "Credit card fraud detection\n",
        "===\n",
        "\n",
        "Real-life classification problems consist of many cases in which the data is not \"beautifully\" balanced. In such cases, one may find the accuracy score of over 90%, but that may not be the happy ending. This is because we often want to focus only on the minority class, as it plays more important role in such problems. In order to tackle this, we need to come up with proper evaluation metrics and resampling techniques. One of the popular problem regarding imbalanced dataset is Credit Card Fraud Detection. Throughout this problem, we can illustrate well what we need to handle such an imbalanced dataset.\n",
        "\n",
        "![Img](https://raw.githubusercontent.com/tranctan96/Credit-Card-Fraud-Detection-Img/master/Credit-card-fraud.jpg)\n",
        "\n",
        "### Understanding the dataset\n",
        "\n",
        "- **Class**: 0 - non-fraud, 1 - fraud\n",
        "- **Amount**: Transaction's amount\n",
        "- **V1,V2...V28**: These are anonymous features, due to confidentiality. Additionally, these are numerical values which are results of PCA transformation.\n",
        "- **Time**: The amount of seconds elapsed between each transaction and the first transaction in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiAvZODVeNV-",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data Analysis\n",
        "Let's import libraries first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1DMRXOqeNWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries: pandas, numpy, matplotlib, seaborn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5al_Vbz3eNWK",
        "colab_type": "text"
      },
      "source": [
        "#### Read csv file \"creditcard.csv\" as data and take a look at dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyV0qolReNWM",
        "colab_type": "code",
        "outputId": "31763a6a-e518-4fea-c34f-878aa1bca862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# Read dataset with extra params sep='\\t', encoding=\"latin-1\"\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "# Your code here\n",
        "data = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/creditcard.csv\")\n",
        "data.head()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4jZ9UWoeNWS",
        "colab_type": "text"
      },
      "source": [
        "#### Get brief information about dataset, how many observations, columns, columns' type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLSC2bHfeNWV",
        "colab_type": "code",
        "outputId": "7ff5eecf-808a-474c-a5bc-2c541914d546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "# Your code here\n",
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            "Time      284807 non-null float64\n",
            "V1        284807 non-null float64\n",
            "V2        284807 non-null float64\n",
            "V3        284807 non-null float64\n",
            "V4        284807 non-null float64\n",
            "V5        284807 non-null float64\n",
            "V6        284807 non-null float64\n",
            "V7        284807 non-null float64\n",
            "V8        284807 non-null float64\n",
            "V9        284807 non-null float64\n",
            "V10       284807 non-null float64\n",
            "V11       284807 non-null float64\n",
            "V12       284807 non-null float64\n",
            "V13       284807 non-null float64\n",
            "V14       284807 non-null float64\n",
            "V15       284807 non-null float64\n",
            "V16       284807 non-null float64\n",
            "V17       284807 non-null float64\n",
            "V18       284807 non-null float64\n",
            "V19       284807 non-null float64\n",
            "V20       284807 non-null float64\n",
            "V21       284807 non-null float64\n",
            "V22       284807 non-null float64\n",
            "V23       284807 non-null float64\n",
            "V24       284807 non-null float64\n",
            "V25       284807 non-null float64\n",
            "V26       284807 non-null float64\n",
            "V27       284807 non-null float64\n",
            "V28       284807 non-null float64\n",
            "Amount    284807 non-null float64\n",
            "Class     284807 non-null int64\n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoypS94qeNWZ",
        "colab_type": "code",
        "outputId": "a72bd22a-27f1-4f6e-ed01-2a3465ff1296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Another way to check for null\n",
        "# hint: isnull()\n",
        "# Your code here\n",
        "data.isnull().any().any()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0VmUGOxeNWd",
        "colab_type": "code",
        "outputId": "f3d7e912-6ec2-49dc-8e9c-c26998e9e21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Statistical view of dataset\n",
        "# Your code here\n",
        "data.describe().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>9.481386e+04</td>\n",
              "      <td>47488.145955</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>54201.500000</td>\n",
              "      <td>84692.000000</td>\n",
              "      <td>139320.500000</td>\n",
              "      <td>172792.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V1</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>3.919560e-15</td>\n",
              "      <td>1.958696</td>\n",
              "      <td>-56.407510</td>\n",
              "      <td>-0.920373</td>\n",
              "      <td>0.018109</td>\n",
              "      <td>1.315642</td>\n",
              "      <td>2.454930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V2</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>5.688174e-16</td>\n",
              "      <td>1.651309</td>\n",
              "      <td>-72.715728</td>\n",
              "      <td>-0.598550</td>\n",
              "      <td>0.065486</td>\n",
              "      <td>0.803724</td>\n",
              "      <td>22.057729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V3</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-8.769071e-15</td>\n",
              "      <td>1.516255</td>\n",
              "      <td>-48.325589</td>\n",
              "      <td>-0.890365</td>\n",
              "      <td>0.179846</td>\n",
              "      <td>1.027196</td>\n",
              "      <td>9.382558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V4</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>2.782312e-15</td>\n",
              "      <td>1.415869</td>\n",
              "      <td>-5.683171</td>\n",
              "      <td>-0.848640</td>\n",
              "      <td>-0.019847</td>\n",
              "      <td>0.743341</td>\n",
              "      <td>16.875344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V5</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-1.552563e-15</td>\n",
              "      <td>1.380247</td>\n",
              "      <td>-113.743307</td>\n",
              "      <td>-0.691597</td>\n",
              "      <td>-0.054336</td>\n",
              "      <td>0.611926</td>\n",
              "      <td>34.801666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V6</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>2.010663e-15</td>\n",
              "      <td>1.332271</td>\n",
              "      <td>-26.160506</td>\n",
              "      <td>-0.768296</td>\n",
              "      <td>-0.274187</td>\n",
              "      <td>0.398565</td>\n",
              "      <td>73.301626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V7</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-1.694249e-15</td>\n",
              "      <td>1.237094</td>\n",
              "      <td>-43.557242</td>\n",
              "      <td>-0.554076</td>\n",
              "      <td>0.040103</td>\n",
              "      <td>0.570436</td>\n",
              "      <td>120.589494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V8</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-1.927028e-16</td>\n",
              "      <td>1.194353</td>\n",
              "      <td>-73.216718</td>\n",
              "      <td>-0.208630</td>\n",
              "      <td>0.022358</td>\n",
              "      <td>0.327346</td>\n",
              "      <td>20.007208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V9</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-3.137024e-15</td>\n",
              "      <td>1.098632</td>\n",
              "      <td>-13.434066</td>\n",
              "      <td>-0.643098</td>\n",
              "      <td>-0.051429</td>\n",
              "      <td>0.597139</td>\n",
              "      <td>15.594995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V10</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>1.768627e-15</td>\n",
              "      <td>1.088850</td>\n",
              "      <td>-24.588262</td>\n",
              "      <td>-0.535426</td>\n",
              "      <td>-0.092917</td>\n",
              "      <td>0.453923</td>\n",
              "      <td>23.745136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V11</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>9.170318e-16</td>\n",
              "      <td>1.020713</td>\n",
              "      <td>-4.797473</td>\n",
              "      <td>-0.762494</td>\n",
              "      <td>-0.032757</td>\n",
              "      <td>0.739593</td>\n",
              "      <td>12.018913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V12</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-1.810658e-15</td>\n",
              "      <td>0.999201</td>\n",
              "      <td>-18.683715</td>\n",
              "      <td>-0.405571</td>\n",
              "      <td>0.140033</td>\n",
              "      <td>0.618238</td>\n",
              "      <td>7.848392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V13</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>1.693438e-15</td>\n",
              "      <td>0.995274</td>\n",
              "      <td>-5.791881</td>\n",
              "      <td>-0.648539</td>\n",
              "      <td>-0.013568</td>\n",
              "      <td>0.662505</td>\n",
              "      <td>7.126883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V14</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>1.479045e-15</td>\n",
              "      <td>0.958596</td>\n",
              "      <td>-19.214325</td>\n",
              "      <td>-0.425574</td>\n",
              "      <td>0.050601</td>\n",
              "      <td>0.493150</td>\n",
              "      <td>10.526766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V15</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>3.482336e-15</td>\n",
              "      <td>0.915316</td>\n",
              "      <td>-4.498945</td>\n",
              "      <td>-0.582884</td>\n",
              "      <td>0.048072</td>\n",
              "      <td>0.648821</td>\n",
              "      <td>8.877742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V16</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>1.392007e-15</td>\n",
              "      <td>0.876253</td>\n",
              "      <td>-14.129855</td>\n",
              "      <td>-0.468037</td>\n",
              "      <td>0.066413</td>\n",
              "      <td>0.523296</td>\n",
              "      <td>17.315112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V17</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-7.528491e-16</td>\n",
              "      <td>0.849337</td>\n",
              "      <td>-25.162799</td>\n",
              "      <td>-0.483748</td>\n",
              "      <td>-0.065676</td>\n",
              "      <td>0.399675</td>\n",
              "      <td>9.253526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V18</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>4.328772e-16</td>\n",
              "      <td>0.838176</td>\n",
              "      <td>-9.498746</td>\n",
              "      <td>-0.498850</td>\n",
              "      <td>-0.003636</td>\n",
              "      <td>0.500807</td>\n",
              "      <td>5.041069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V19</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>9.049732e-16</td>\n",
              "      <td>0.814041</td>\n",
              "      <td>-7.213527</td>\n",
              "      <td>-0.456299</td>\n",
              "      <td>0.003735</td>\n",
              "      <td>0.458949</td>\n",
              "      <td>5.591971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V20</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>5.085503e-16</td>\n",
              "      <td>0.770925</td>\n",
              "      <td>-54.497720</td>\n",
              "      <td>-0.211721</td>\n",
              "      <td>-0.062481</td>\n",
              "      <td>0.133041</td>\n",
              "      <td>39.420904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V21</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>1.537294e-16</td>\n",
              "      <td>0.734524</td>\n",
              "      <td>-34.830382</td>\n",
              "      <td>-0.228395</td>\n",
              "      <td>-0.029450</td>\n",
              "      <td>0.186377</td>\n",
              "      <td>27.202839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V22</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>7.959909e-16</td>\n",
              "      <td>0.725702</td>\n",
              "      <td>-10.933144</td>\n",
              "      <td>-0.542350</td>\n",
              "      <td>0.006782</td>\n",
              "      <td>0.528554</td>\n",
              "      <td>10.503090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V23</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>5.367590e-16</td>\n",
              "      <td>0.624460</td>\n",
              "      <td>-44.807735</td>\n",
              "      <td>-0.161846</td>\n",
              "      <td>-0.011193</td>\n",
              "      <td>0.147642</td>\n",
              "      <td>22.528412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V24</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>4.458112e-15</td>\n",
              "      <td>0.605647</td>\n",
              "      <td>-2.836627</td>\n",
              "      <td>-0.354586</td>\n",
              "      <td>0.040976</td>\n",
              "      <td>0.439527</td>\n",
              "      <td>4.584549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V25</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>1.453003e-15</td>\n",
              "      <td>0.521278</td>\n",
              "      <td>-10.295397</td>\n",
              "      <td>-0.317145</td>\n",
              "      <td>0.016594</td>\n",
              "      <td>0.350716</td>\n",
              "      <td>7.519589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V26</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>1.699104e-15</td>\n",
              "      <td>0.482227</td>\n",
              "      <td>-2.604551</td>\n",
              "      <td>-0.326984</td>\n",
              "      <td>-0.052139</td>\n",
              "      <td>0.240952</td>\n",
              "      <td>3.517346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V27</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-3.660161e-16</td>\n",
              "      <td>0.403632</td>\n",
              "      <td>-22.565679</td>\n",
              "      <td>-0.070840</td>\n",
              "      <td>0.001342</td>\n",
              "      <td>0.091045</td>\n",
              "      <td>31.612198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V28</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>-1.206049e-16</td>\n",
              "      <td>0.330083</td>\n",
              "      <td>-15.430084</td>\n",
              "      <td>-0.052960</td>\n",
              "      <td>0.011244</td>\n",
              "      <td>0.078280</td>\n",
              "      <td>33.847808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amount</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>8.834962e+01</td>\n",
              "      <td>250.120109</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>77.165000</td>\n",
              "      <td>25691.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <td>284807.0</td>\n",
              "      <td>1.727486e-03</td>\n",
              "      <td>0.041527</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           count          mean  ...            75%            max\n",
              "Time    284807.0  9.481386e+04  ...  139320.500000  172792.000000\n",
              "V1      284807.0  3.919560e-15  ...       1.315642       2.454930\n",
              "V2      284807.0  5.688174e-16  ...       0.803724      22.057729\n",
              "V3      284807.0 -8.769071e-15  ...       1.027196       9.382558\n",
              "V4      284807.0  2.782312e-15  ...       0.743341      16.875344\n",
              "V5      284807.0 -1.552563e-15  ...       0.611926      34.801666\n",
              "V6      284807.0  2.010663e-15  ...       0.398565      73.301626\n",
              "V7      284807.0 -1.694249e-15  ...       0.570436     120.589494\n",
              "V8      284807.0 -1.927028e-16  ...       0.327346      20.007208\n",
              "V9      284807.0 -3.137024e-15  ...       0.597139      15.594995\n",
              "V10     284807.0  1.768627e-15  ...       0.453923      23.745136\n",
              "V11     284807.0  9.170318e-16  ...       0.739593      12.018913\n",
              "V12     284807.0 -1.810658e-15  ...       0.618238       7.848392\n",
              "V13     284807.0  1.693438e-15  ...       0.662505       7.126883\n",
              "V14     284807.0  1.479045e-15  ...       0.493150      10.526766\n",
              "V15     284807.0  3.482336e-15  ...       0.648821       8.877742\n",
              "V16     284807.0  1.392007e-15  ...       0.523296      17.315112\n",
              "V17     284807.0 -7.528491e-16  ...       0.399675       9.253526\n",
              "V18     284807.0  4.328772e-16  ...       0.500807       5.041069\n",
              "V19     284807.0  9.049732e-16  ...       0.458949       5.591971\n",
              "V20     284807.0  5.085503e-16  ...       0.133041      39.420904\n",
              "V21     284807.0  1.537294e-16  ...       0.186377      27.202839\n",
              "V22     284807.0  7.959909e-16  ...       0.528554      10.503090\n",
              "V23     284807.0  5.367590e-16  ...       0.147642      22.528412\n",
              "V24     284807.0  4.458112e-15  ...       0.439527       4.584549\n",
              "V25     284807.0  1.453003e-15  ...       0.350716       7.519589\n",
              "V26     284807.0  1.699104e-15  ...       0.240952       3.517346\n",
              "V27     284807.0 -3.660161e-16  ...       0.091045      31.612198\n",
              "V28     284807.0 -1.206049e-16  ...       0.078280      33.847808\n",
              "Amount  284807.0  8.834962e+01  ...      77.165000   25691.160000\n",
              "Class   284807.0  1.727486e-03  ...       0.000000       1.000000\n",
              "\n",
              "[31 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewi85lU3eNWg",
        "colab_type": "text"
      },
      "source": [
        "## 2. Explore Data Analysis\n",
        "### Visualization & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fpwchuieNWh",
        "colab_type": "text"
      },
      "source": [
        "#### Remember the anonymized predictors that is already transformed with PCA ?\n",
        "Let check it, plot histogram for **'V1', 'V2', 'Time', 'Amount'** columns\n",
        "\n",
        "hint: \n",
        "1. Use sns.distplot\n",
        "2. Multiple plot\n",
        "https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html#plt.subplot:-Simple-Grids-of-Subplots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipXto29GeNWj",
        "colab_type": "code",
        "outputId": "91db3c52-8090-4152-f3ec-eb35d9d4a006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "gs = gridspec.GridSpec(2,2)\n",
        "#### Remember the anonymized predictors that is already transformed with PCA ?\n",
        "cols=['V1','V2','Time','Account']\n",
        "plt.figure(figsize=(15, 10)) # Set figsize\n",
        "\n",
        "for idx,col in enumerate(cols):\n",
        "  sns.distplot(data[col],)\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Account'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a1346c12596f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Account'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAJQCAYAAADG2iYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d-_hgUzeNWq",
        "colab_type": "text"
      },
      "source": [
        "Here we still have two predictors **Amount** and **Time** that has not been scaled yet. So we need to normalize these features also. Using **sklearn.preprocessing.StandardScaler** for this task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wGRqyEeeNWr",
        "colab_type": "code",
        "outputId": "4008d49d-a3f8-4322-daa0-2a86201f60a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# create StandardScaler and assign to std_scaler\n",
        "std_scaler = StandardScaler()\n",
        "# std_scaler.fit([data['Amount']])\n",
        "# std_scaler.fit([data['Time']])\n",
        "# fit_transform \"Amount\", \"Time\" columns and replace old values with transformed values\n",
        "data['Amount']= std_scaler.fit_transform(data[['Amount']])\n",
        "data['Time']= std_scaler.fit_transform(data[['Time']])\n",
        "data.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.996583</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>0.244964</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.996583</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>-0.342475</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.996562</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>1.160686</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.996562</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>0.140534</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.996541</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>-0.073403</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Time        V1        V2        V3  ...       V27       V28    Amount  Class\n",
              "0 -1.996583 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  0.244964      0\n",
              "1 -1.996583  1.191857  0.266151  0.166480  ... -0.008983  0.014724 -0.342475      0\n",
              "2 -1.996562 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  1.160686      0\n",
              "3 -1.996562 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  0.140534      0\n",
              "4 -1.996541 -1.158233  0.877737  1.548718  ...  0.219422  0.215153 -0.073403      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqWPDf2HeNWv",
        "colab_type": "text"
      },
      "source": [
        "### plot histogram for 'V1', 'V2', 'Time', 'Amount' columns again to check \n",
        "\n",
        "1.   lment de liste\n",
        "2.   lment de liste\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcztaL7UkDv9",
        "colab_type": "code",
        "outputId": "d6f0e7ef-d1b9-416c-d537-328f8689bd8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# plt.figure(figsize=(15, 10)) # Set figsize\n",
        "# for i in range(1, 5):\n",
        "#     plt.subplot(data['V1']\n",
        "#     plt.text(0.5, 0.5, str((2, 3, i)),\n",
        "#              fontsize=18, ha='center')\n",
        "\n",
        "# plt.show()\n",
        "# fig, axs = plt.subplots(1,4)\n",
        "sns.distplot(data['V1'])\n",
        "plt.show()\n",
        "sns.distplot(data['V2'])\n",
        "plt.show()\n",
        "sns.distplot(data['Time'])\n",
        "plt.show()\n",
        "sns.distplot(data['Amount'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfRGRcURBX0RkHFHQFxEZRxT0RUTGkf8P9JkzEPKsNTQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQX7sKRsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEPNJREFUeJzt3XuwnHV9x/H3JzcIcifBSxJM1HgJ\n3rARcKReKDMF7RBnijOJtrWtMxk7olidtnTsMC2d6RTr0NaWcaReS0UUtDXVKFrAqu1wCcr9ZgxI\nEkQSLuHWEEK+/WOf4HJMOBuyJ5vz4/2ayZx9nv2d3d+TJ+d9dp/dfZKqQpLUlimjnoAkafiMuyQ1\nyLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOmjeqOZ82aVfPnzx/V3UvSpHT11VdvrKrZ\n440bWdznz5/PqlWrRnX3kjQpJfnZIOM8LCNJDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLu\nktQg4y5JDRrZJ1R3x/lX3LnT6951zBF7cCaStHfykbskNci4S1KDjLskNci4S1KDjLskNci4S1KD\njLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLsk\nNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNWiguCc5McmtSVYnOf1pxv12kkqy\neHhTlCTtqnHjnmQqcA5wErAIWJZk0Q7GHQCcBlwx7ElKknbNII/cjwZWV9WaqtoCXAAs2cG4vwbO\nAjYPcX6SpGdgkLjPAdb2La/r1j0pyeuAeVX1zae7oSTLk6xKsmrDhg27PFlJ0mB2+wXVJFOAs4GP\njDe2qs6tqsVVtXj27Nm7e9eSpJ0YJO7rgXl9y3O7ddsdALwS+F6SO4BjgRW+qCpJozNI3K8CFiZZ\nkGQGsBRYsf3KqtpUVbOqan5VzQcuB06uqlUTMmNJ0rjGjXtVbQVOBS4Gbga+UlU3JjkzyckTPUFJ\n0q6bNsigqloJrByz7oydjH3L7k9LkrQ7/ISqJDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXI\nuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtS\ng4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7\nJDXIuEtSgwaKe5ITk9yaZHWS03dw/fuSXJ/kmiQ/TLJo+FOVJA1q3LgnmQqcA5wELAKW7SDe51fV\nq6rqtcDHgLOHPlNJ0sAGeeR+NLC6qtZU1RbgAmBJ/4CqerBv8TlADW+KkqRdNW2AMXOAtX3L64Bj\nxg5K8n7gw8AM4PihzE6S9IwM7QXVqjqnql4M/BnwFzsak2R5klVJVm3YsGFYdy1JGmOQuK8H5vUt\nz+3W7cwFwDt2dEVVnVtVi6tq8ezZswefpSRplwwS96uAhUkWJJkBLAVW9A9IsrBv8e3AT4Y3RUnS\nrhr3mHtVbU1yKnAxMBX4bFXdmORMYFVVrQBOTXIC8DhwP/CeiZy0JOnpDfKCKlW1Elg5Zt0ZfZdP\nG/K8JEm7wU+oSlKDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KD\njLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLsk\nNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDBop7khOT\n3JpkdZLTd3D9h5PclOS6JJckeeHwpypJGtS4cU8yFTgHOAlYBCxLsmjMsB8Di6vq1cBFwMeGPVFJ\n0uAGeeR+NLC6qtZU1RbgAmBJ/4CquqyqHu0WLwfmDneakqRdMUjc5wBr+5bXdet25r3At3ZnUpKk\n3TNtmDeW5HeAxcCbd3L9cmA5wBFHHDHMu5Yk9Rnkkft6YF7f8txu3VMkOQH4KHByVT22oxuqqnOr\nanFVLZ49e/Yzma8kaQCDxP0qYGGSBUlmAEuBFf0DkhwFfIpe2O8Z/jQlSbti3LhX1VbgVOBi4Gbg\nK1V1Y5Izk5zcDfs7YH/gwiTXJFmxk5uTJO0BAx1zr6qVwMox687ou3zCkOclSdoNfkJVkhpk3CWp\nQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZd\nkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk\n3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkho0UNyTnJjk1iSrk5y+g+vflORHSbYm\nOWX405Qk7Ypx455kKnAOcBKwCFiWZNGYYXcCvw+cP+wJSpJ23bQBxhwNrK6qNQBJLgCWADdtH1BV\nd3TXbZuAOUqSdtEgh2XmAGv7ltd16yRJe6k9+oJqkuVJViVZtWHDhj1515L0rDJI3NcD8/qW53br\ndllVnVtVi6tq8ezZs5/JTUiSBjBI3K8CFiZZkGQGsBRYMbHTkiTtjnHjXlVbgVOBi4Gbga9U1Y1J\nzkxyMkCS1ydZB7wT+FSSGydy0pKkpzfIu2WoqpXAyjHrzui7fBW9wzWSpL2An1CVpAYZd0lqkHGX\npAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZ\nd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lq\nkHGXpAYZd0lqUBNxv/fhx/jGdXfxqe//lOPOupTbfvHQqKckSSPVRNwvvHodV95+H1XwwKOP8/GL\nbx31lCRppKaNegK768HNj3PnfY9ywisO5/iXP5eNDz/G2d+9jWvXPsBr5h086ulJ0khM+kfuN931\nIABHvuAgAP7wuAUcst90Pv4dH71Levaa9I/cb7rrQWbtP4PDD9gHgBXX3MWxLzqMb91wN2f+5028\n5PD9AXjXMUeMcpqStEcN9Mg9yYlJbk2yOsnpO7h+nyRf7q6/Isn8YU90Rx7dspU1Gx9m0fMPIsmT\n64990WEcNHM6n/uf2/m3y3/Gmo0PU1V7YkqStFcYN+5JpgLnACcBi4BlSRaNGfZe4P6qegnw98BZ\nw57ojtxy90NsKzjyBQc+Zf30qVP4oze/mDe9dDa3b3yET//gdt7+iR9y4aq13L1ps6GX1LxBDssc\nDayuqjUASS4AlgA39Y1ZAvxld/ki4J+TpCagopfe8gvOv/JOFj3/AK5d+wAHzZzOnENm/sq4A2dO\n5zePfB5vfdnhXLv2AW64axN/ctF1vev2ncaC2fsz5+B9ed6BMzl4v+kcuO80Dpw5nQP3nd77OnMa\nB+w7nelTQhKmhB1+BQiQQOiWf/kk4sn129f1xo75vv5vkLTX2ratnvIzu21b8djWbUyfGqZNnUJV\n8eiWJ3hky1b232caM6dPZfPj21j/wKM8tHkrcw6eyaz992HKlIn/mR8k7nOAtX3L64BjdjamqrYm\n2QQcBmwcxiT7bXx4C3dsfIQb1m8CeodgpjxNHGdMm8LrFxzK4vmHcOd9j3LXps3c8+Bm7n1kC1fe\nfj8P/t89bHli27Cn+Yz1fhlsv5y+y90vj6f8kth+OU/5vt3l85o2DeuhVg3hX8hEPnkem4OM+ckY\ne31Vb5u2FVQVVbCtessJTJ8yhalTwuNPbGPrtt7EZ0ydQgKPbf1lO6ZPDVU8OQZg2pQ8ZXn79565\n5EiWHj2xrwPu0RdUkywHlneLDyfZ3be0zPoZbPzybt7IJDGLCfhluZdyW9v0bNpWeJrtXfY3sOyZ\n3+4LBxk0SNzXA/P6lud263Y0Zl2SacBBwL1jb6iqzgXOHWRig0iyqqoWD+v29mZua5vc1naNensH\nebfMVcDCJAuSzACWAivGjFkBvKe7fApw6UQcb5ckDWbcR+7dMfRTgYuBqcBnq+rGJGcCq6pqBfAZ\n4Lwkq4H76P0CkCSNyEDH3KtqJbByzLoz+i5vBt453KkNZGiHeCYBt7VNbmu7Rrq98eiJJLVn0p9b\nRpL0qyZl3Mc7HcJklmReksuS3JTkxiSndesPTfLdJD/pvh4y6rkOS5KpSX6c5Bvd8oLuNBaru9Na\nzBj1HIclycFJLkpyS5Kbk7yh1X2b5I+7f8M3JPlSkn1b2bdJPpvkniQ39K3b4X5Mzye6bb4uyev2\nxBwnXdwHPB3CZLYV+EhVLQKOBd7fbd/pwCVVtRC4pFtuxWnAzX3LZwF/353O4n56p7doxT8C366q\nlwOvobfdze3bJHOADwKLq+qV9N6MsZR29u3ngRPHrNvZfjwJWNj9WQ58ck9McNLFnb7TIVTVFmD7\n6RCaUFU/r6ofdZcfovfDP4feNn6hG/YF4B2jmeFwJZkLvB34dLcc4Hh6p7GAtrb1IOBN9N5dRlVt\nqaoHaHTf0nvDxszusy/7AT+nkX1bVd+n987Afjvbj0uAf62ey4GDkzx/ouc4GeO+o9MhzBnRXCZU\nd3bNo4ArgOdW1c+7q+4GnjuiaQ3bPwB/Cmz/HPdhwANVtbVbbmn/LgA2AJ/rDkN9OslzaHDfVtV6\n4OPAnfSivgm4mnb3Lex8P46kWZMx7s8KSfYHvgp8qKoe7L+u+4DYpH+bU5LfAu6pqqtHPZc9ZBrw\nOuCTVXUU8AhjDsE0tG8PofeIdQHwAuA5/OphjGbtDftxMsZ9kNMhTGpJptML+xer6mvd6l9sfyrX\nfb1nVPMbojcCJye5g97htePpHZM+uHsqD23t33XAuqq6olu+iF7sW9y3JwC3V9WGqnoc+Bq9/d3q\nvoWd78eRNGsyxn2Q0yFMWt0x588AN1fV2X1X9Z/i4T3A1/f03Iatqv68quZW1Xx6+/HSqno3cBm9\n01hAI9sKUFV3A2uTvKxb9Rv0Tp3d3L6ldzjm2CT7df+mt29rk/u2s7P9uAL4ve5dM8cCm/oO30yc\n3ikuJ9cf4G3AbcBPgY+Oej5D3rbj6D2duw64pvvzNnrHoi8BfgL8F3DoqOc65O1+C/CN7vKLgCuB\n1cCFwD6jnt8Qt/O1wKpu//4HcEir+xb4K+AW4AbgPGCfVvYt8CV6ryU8Tu8Z2Xt3th/pnY37nK5X\n19N7B9GEz9FPqEpSgybjYRlJ0jiMuyQ1yLhLUoOMuyQ1yLhLUoOMu5qS5B1JKsnLRziHDyXZb1T3\nL4FxV3uWAT9kt/5z+d32IXonypJGxrirGd35eI6j94GSpd26tyT57yRfT7Imyd8meXeSK5Ncn+TF\n3bj5SS7tzrd9SZIjuvWfT3JK33083He73+s7N/sXu08gfpDeuVQuS3LZHv4rkJ5k3NWSJfTOlX4b\ncG+SX+vWvwZ4H/AK4HeBl1bV0fROM/yBbsw/AV+oqlcDXwQ+McD9HUXvUfoiep+8fGNVfQK4C3hr\nVb11OJsl7TrjrpYso3cCMrqv2w/NXFW98+Q/Ru8j4N/p1l8PzO8uvwE4v7t8Hr1nAOO5sqrWVdU2\neqeJmD/OeGmPmTb+EGnvl+RQemeVfFWSovc//xTwTeCxvqHb+pa3Mf7PwFa6B0FJpgD9/y1c/+0+\nMcBtSXuMj9zVilOA86rqhVU1v6rmAbcDvz7g9/8v3XF64N3AD7rLdwDbD++cDEwf4LYeAg4Y8H6l\nCWHc1YplwL+PWfdVBn/XzAeAP0hyHb3j8qd16/8FeHOSa+kdunlkgNs6F/i2L6hqlDwrpCQ1yEfu\nktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDfp/6kMaBnDiL70AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2r-hW-GeNWw",
        "colab_type": "code",
        "outputId": "89807238-a8cf-49fc-fb19-cbe9a02ac8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "plt.figure(figsize=(15, 10)) # Set figsize\n",
        "# Your code here\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x720 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Z3wTqPeNW4",
        "colab_type": "text"
      },
      "source": [
        "### Plotting target classes\n",
        "In order to check how skewed our data is, let's check for the percentage of each class:\n",
        "\n",
        "Hint: Base on \"Class\" columns\n",
        "- 0: No Fraud\n",
        "- 1: Fraud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trb9w4EveNW7",
        "colab_type": "code",
        "outputId": "3a24c8cb-bf58-440e-98f9-9a175a336ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Check ratio between classes\n",
        "percentage_fraud =len(data[data['Class']==1])/len(data) # Your code here: number of fraud / total of observation\n",
        "percentage_no_fraud =len(data[data['Class']==0])/len(data) # Your code here: number of no fraud / total of observation\n",
        "\n",
        "print ('Percentage Fraud transactions: ', percentage_fraud)\n",
        "print ('Percentage No-fraud transactions: ', percentage_no_fraud)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage Fraud transactions:  0.001727485630620034\n",
            "Percentage No-fraud transactions:  0.9982725143693799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3f1EM28eNW-",
        "colab_type": "text"
      },
      "source": [
        "#### Easily find out, this dataset is heavily skewed.\n",
        "#### Data is beautiful, so let's plot it to visualize the skewness:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD-GddSbeNW_",
        "colab_type": "code",
        "outputId": "9bac8e12-391e-456a-a654-9d2d74080a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "fig = plt.figure(figsize=(7,7)) # Set figsize\n",
        "sns.countplot(data['Class'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAGtCAYAAABN1p4cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEwSUwCYJKYAMElMAWCSmALApP8Hx5M7s3Rk7WQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLjKE2B8eNXD",
        "colab_type": "text"
      },
      "source": [
        "### Approaches to handle imbalanced data\n",
        "Refs: https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/\n",
        "\n",
        "Dealing with imbalanced datasets entails strategies such as improving classification algorithms or balancing classes in the training data (data resampling) before providing the data as input to the machine learning algorithm. The later technique is preferred as it has wider application.\n",
        "\n",
        "#### Data-level techniques (Data resampling):\n",
        "Refs: \n",
        "- https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
        "- https://beckernick.github.io/oversampling-modeling/\n",
        "\n",
        "Despite the advantage of balancing classes, these techniques also have their weaknesses. The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. Whereas in under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n",
        "![alt text](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png \"Resampling methods\")\n",
        "\n",
        "- Random over-sampling\n",
        "- Random under-sampling\n",
        "- SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "- ...\n",
        "\n",
        "#### Algorithm-level techniques:\n",
        "- Algorithmic ensemble techniques\n",
        "    - Bagging-based \n",
        "    - Boosting-based\n",
        "    - Adaptive Boosting - Ada Boost\n",
        "    - Gradient Tree Boosting\n",
        "    - XG Boost\n",
        "    - ...\n",
        "\n",
        "#### Improving classification algorithm\n",
        "- Using built-in **class_weight** option from sklearn models:\n",
        "    - Logistic regresion\n",
        "    - [SVM](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html)\n",
        "    - ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ_xLBZZeNXE",
        "colab_type": "text"
      },
      "source": [
        "## Assigning X and y for Original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_oCcSgBeNXG",
        "colab_type": "code",
        "outputId": "82736cf2-121f-4dd3-9cdb-cafd6a671d0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Original data\n",
        "X = data.drop(columns=\"Class\")# Your code here: Keep all columns except for \"Class\"\n",
        "y = data['Class']# Your code here: \"Class\" column\n",
        "\n",
        "print ('X shape:', X.shape)\n",
        "print ('y shape:', y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (284807, 30)\n",
            "y shape: (284807,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbJ0glNveNXO",
        "colab_type": "text"
      },
      "source": [
        "#### Train/Test split original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzCrf5tkeNXR",
        "colab_type": "code",
        "outputId": "46528c46-3bb7-4db0-b2dc-29139b317fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Your code here\n",
        "\n",
        "# Whole dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)\n",
        "\n",
        "print(\"Number transactions training dataset: \", len(X_train))\n",
        "print(\"Number transactions testing dataset: \", len(X_test))\n",
        "print(\"Total number of transactions: \", len(X_train)+len(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number transactions training dataset:  199364\n",
            "Number transactions testing dataset:  85443\n",
            "Total number of transactions:  284807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isINgU27eNXV",
        "colab_type": "text"
      },
      "source": [
        "## Obtaining smaller training data\n",
        "\n",
        "As the total number of transactions is too large, which may damage your computer. We obtain the smaller training dataset with the same ratio of classes of original training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3FHK6pzeNXY",
        "colab_type": "code",
        "outputId": "09031418-cc38-4b83-c748-9d3c217fc2e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "training_data = pd.concat ([X_train,y_train],axis = 1)\n",
        "training_data['Class'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    199016\n",
              "1       348\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd97EEeIeNXc",
        "colab_type": "code",
        "outputId": "d5768e5b-7dd0-4fa0-96fa-ab1a1acfb078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print ('Percentage original fraud: ', percentage_fraud)\n",
        "print ('Percentage original no-fraud: ', percentage_no_fraud)\n",
        "number_of_instances = 100000\n",
        "# We will obtain maximum 100.000 data instances with the same class ratio of original data.\n",
        "# Therefore, new data will have 0.17% fraud and 99.83% non-fraud of 100.000.\n",
        "# Which means, new data will have 170 fraud transactions and 99830 non-fraud transactions.\n",
        "\n",
        "number_sub_fraud = int (percentage_fraud*100/100 * number_of_instances)\n",
        "number_sub_non_fraud = int (percentage_no_fraud*100/100 * number_of_instances)\n",
        "\n",
        "sub_fraud_data = training_data[training_data['Class'] == 1].head(number_sub_fraud)\n",
        "sub_non_fraud_data = training_data[training_data['Class'] == 0].head(number_sub_non_fraud)\n",
        "\n",
        "print ('Number of newly sub fraud data:',len(sub_fraud_data))\n",
        "print ('Number of newly sub non-fraud data:',len(sub_non_fraud_data))\n",
        "\n",
        "sub_training_data = pd.concat ([sub_fraud_data, sub_non_fraud_data], axis = 0)\n",
        "sub_training_data['Class'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage original fraud:  0.001727485630620034\n",
            "Percentage original no-fraud:  0.9982725143693799\n",
            "Number of newly sub fraud data: 172\n",
            "Number of newly sub non-fraud data: 99827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    99827\n",
              "1      172\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvXYfdASeNXi",
        "colab_type": "text"
      },
      "source": [
        "#### Randomly Under-Sampling the Training Dataset\n",
        "For simplicity, i use DataFrame.sample() to randomly sample the instances of each class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgYGmUeqeNXi",
        "colab_type": "code",
        "outputId": "5160fd24-f223-4e8f-e87e-c45c136882de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "# Fraud/non-fraud data\n",
        "# Select row which \"Class\" is 1 and save in fraud_data\n",
        "fraud_data = training_data[training_data['Class']==1]\n",
        "# Select row which \"Class\" is 0 and save in non_fraud_data\n",
        "non_fraud_data = training_data[training_data['Class']==0]\n",
        "\n",
        "# Number of fraud, non-fraud transactions\n",
        "number_records_fraud =fraud_data.shape[0]\n",
        "number_records_non_fraud =non_fraud_data.shape[0]\n",
        "\n",
        "# Using sample function on data frame to randomly select number_records_fraud from non_fraud_data data frame\n",
        "under_sample_non_fraud = non_fraud_data.sample(number_records_fraud)\n",
        "# **concat** under_sample_non_fraud and fraud_data to form under_sample_data\n",
        "under_sample_data = pd.concat ([ fraud_data,under_sample_non_fraud], axis = 0)\n",
        "\n",
        "# Showing ratio\n",
        "print(\"Percentage of normal transactions: \"+str(len(under_sample_non_fraud)/(len(under_sample_non_fraud)+len(fraud_data)) ))\n",
        "print(\"Percentage of fraud transactions: \"+ str(len(fraud_data)/(len(under_sample_non_fraud)+len(fraud_data)) ))\n",
        "print(\"Total number of transactions in resampled data: \"+ str(len(under_sample_non_fraud)+len(fraud_data)) )\n",
        "\n",
        "# Assigning X,y for Under-sampled Data\n",
        "X_train_undersample = under_sample_data.drop(columns=\"Class\")\n",
        "y_train_undersample = under_sample_data['Class']\n",
        "\n",
        "# Plot countplot\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.countplot(under_sample_data['Class'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of normal transactions: 0.5\n",
            "Percentage of fraud transactions: 0.5\n",
            "Total number of transactions in resampled data: 696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAGtCAYAAACBT6T3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2BtqKPfeNXq",
        "colab_type": "text"
      },
      "source": [
        "#### Randomly Over-Sampling the Training Dataset\n",
        "I do the same with over-sampling technique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_gy-rfGeNXs",
        "colab_type": "code",
        "outputId": "66436c97-84a8-4335-d38f-390307ce64ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "# Fraud/non-fraud data\n",
        "# Select row which \"Class\" is 1 and save in fraud_data\n",
        "fraud_data = sub_training_data[sub_training_data['Class']==1]\n",
        "# Select row which \"Class\" is 0 and save in non_fraud_data\n",
        "non_fraud_data = sub_training_data[sub_training_data['Class']==0]\n",
        "\n",
        "# Number of fraud, non-fraud transactions\n",
        "number_records_fraud = fraud_data.shape[0]\n",
        "number_records_non_fraud = non_fraud_data.shape[0]\n",
        "\n",
        "# Using sample on fraud_data with replacement \"replace = True\",  since we take a larger sample than population\n",
        "over_sample_fraud = fraud_data.sample(number_records_non_fraud,replace=True)\n",
        "# **concat** over_sample_fraud and non_fraud_data to form under_sample_data\n",
        "over_sample_data =pd.concat([non_fraud_data,over_sample_fraud])\n",
        "\n",
        "# Showing ratio\n",
        "print(\"Percentage of normal transactions: \",1- over_sample_data['Class'].mean())\n",
        "print(\"Percentage of fraud transactions: \", over_sample_data['Class'].mean())\n",
        "print(\"Total number of transactions in resampled data: \",over_sample_data.shape[0])\n",
        "\n",
        "# Assigning X, y for over-sampled dataset\n",
        "X_train_oversample = over_sample_data.drop(columns=[\"Class\"])\n",
        "y_train_oversample = over_sample_data['Class']\n",
        "\n",
        "# Plot countplot\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.countplot(over_sample_data['Class'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of normal transactions:  0.5\n",
            "Percentage of fraud transactions:  0.5\n",
            "Total number of transactions in resampled data:  199654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAGtCAYAAABN1p4cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmALAJDEFgEliCgCTxBQAJokpAEz6/wuh06nlOA10AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1vq9gn7eNX0",
        "colab_type": "text"
      },
      "source": [
        "## 3. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAnYi6SIeNX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spot check with LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, BernoulliNB, GaussianNB, SVM\n",
        "# Import lib\n",
        "# Your code here\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import BernoulliNB\n",
        "# from sklearn.model_selection import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "lr = LogisticRegression() # Your code here: create logistic regression model\n",
        "dtc = DecisionTreeClassifier()# Your code here: create decision tree model\n",
        "rfc = RandomForestClassifier()# Your code here: create random forest model\n",
        "# bnb = BernoulliNB()# Your code here: create BernoulliNB model\n",
        "# gnb = GaussianNB# Your code here: create GaussianNB model\n",
        "svm = SVC()# Your code here: create SVM model\n",
        "\n",
        "models = [lr, dtc, rfc, svm]\n",
        "models_name = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsZ6psYXeNX8",
        "colab_type": "text"
      },
      "source": [
        "## 4. Evaluation Metrics\n",
        "This is a clear example where using a typical accuracy score to evaluate our classification algorithm. For example, if we just used a majority class to assign values to all records, we will still be having a high accuracy, BUT WE WOULD BE CLASSIFYING ALL \"1\" INCORRECTLY!\n",
        "\n",
        "\n",
        "[Choosing the right metrics for classification problems](https://medium.com/usf-msds/choosing-the-right-metric-for-evaluating-machine-learning-models-part-2-86d5649a5428)\n",
        "\n",
        "[Evaluation metrics for classification problems](https://medium.com/greyatom/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b)\n",
        "\n",
        "[What should be used for evaluating a model on an imbalanced data set ?](https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba)\n",
        "- Precision + Recall\n",
        "- ROC = TPR + FPR\n",
        "\n",
        "Briefly explained: \n",
        "- **Precision/Recall** focus more on **Positive** class than **Negative** class.\n",
        "- **TPR/FPR (ROC metrics)** measure the ability to distinguish between the classes of model.\n",
        "\n",
        "- The higher the **Precision/Recall**, the better the ability of predicting **Positive** class of the model. \n",
        "- **TPR** is actually **Recall**.\n",
        "- **FPR** is actually **Low Recall** for **Negative** class. The higher the **FPR**, the worse the ability of predicting **Negative** class of the model (which is not a good model).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGsjXsHReNX9",
        "colab_type": "text"
      },
      "source": [
        "### Precision, Recall, F1-score\n",
        "\n",
        "We are very interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions. If you think how Accuracy, Precision and Recall work for a confusion matrix, recall would be the most interesting:\n",
        "\n",
        "- **Accuracy** = (TP+TN)/total\n",
        "- **Precision** = TP/(TP+FP)\n",
        "- **Recall** = TP/(TP+FN)\n",
        "- **F1 score** = Harmonic Mean (Precision, Recall) = 2 x Precision x Recall / (Precision + Recall). \n",
        "\n",
        "_Harmonic mean is kind of an average when x and y are equal. But either x or y is smaller, then it is closer to the smaller number as compared to the larger number_. **Ex**: _If x is large and y is small, then arithmetic mean (x+y)/2 is the value between. But harmonic mean 2xy/(x+y) is the lower value._\n",
        "\n",
        "**F1-score vs Accuracy**: F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).\n",
        "\n",
        "- **Positives** = Fraud transactions (the minority class)\n",
        "- **Negatives** = Normal transactions\n",
        "\n",
        "\n",
        "- A **true positive** is an outcome where the model correctly predicts the positive class. Similarly, a **true negative** is an outcome where the model correctly predicts the negative class.\n",
        "- A **false positive** is an outcome where the model incorrectly predicts the positive class. And a **false negative** is an outcome where the model incorrectly predicts the negative class.\n",
        "\n",
        "\n",
        "- **TP**: actually Fraud and predicted as Fraud\n",
        "- **FP**: actually Normal but predicted as Fraud\n",
        "- **TN**: actually Normal and predicted as Normal\n",
        "- **FN**: actually Fraud but predicted as Normal\n",
        "\n",
        "Due to the imbalance issue, many observations can be predicted as Normal transaction, whereas they are acutally Fraud transactions, which is **False Negatives**. **Recall** penalize the False Negatives.\n",
        "\n",
        "Obviously, trying to increase recall, tends to come with a decrease of precision. However, in our case, if we predict that a transaction is fraudulent and turns out not to be, is not a massive problem compared to the opposite situation.\n",
        "\n",
        "- **Precision** = percentage of correctly classified (actual) Fraud transactions over all transactions that are classified/predicted as Fraud. _**Ex**: Our model has a precision of 0.5in other words, when it predicts a transaction is Fraud, 50% it is correct._\n",
        "- **Recall** = percentage of correctly classified (actual) Fraud transactions over all transactions that are actually Fraud. _**Ex**: Our model has a recall of 0.11  in other words, it correctly identifies 11% of all Fraud transactions in the dataset._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02DXL3wReNX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import confusion_matrix, classification_report\n",
        "# Your code here\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "# We create an utils function, that take a trained model as argument and print out confusion matrix\n",
        "# classification report base on X and y\n",
        "def evaluate_model(estimator, X, y):\n",
        "    # Note: We should test on the original test set\n",
        "    estimator.fit(X,y)\n",
        "    prediction=estimator.predict(X_test)\n",
        "    print('Confusion matrix:\\n', confusion_matrix(y_test,prediction))\n",
        "    print('Classification report:\\n', classification_report(y_test,prediction))\n",
        "    print('Testing set information:\\n',y.value_counts())\n",
        "\n",
        "    # Set print options\n",
        "    np.set_printoptions(precision=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE1WfX2eeNYA",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate with _Origin_ dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljOVj76CeNYB",
        "colab_type": "code",
        "outputId": "1772ac5b-f4c4-4ee1-86f1-292ac6d018dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Now we will test on origin dataset (X_train_sub, y_train_sub)\n",
        "# We loop for models\n",
        "# For each model, we train with train_sub dataset\n",
        "# and use evaluate_model function to test with test set\n",
        "X_train_sub=sub_training_data.drop(columns=['Class'])\n",
        "y_train_sub=sub_training_data['Class']\n",
        "for idx, model in enumerate(models):\n",
        "    print(\"Model: {}\".format(models_name[idx]))\n",
        "    evaluate_model(model,X_train_sub,y_train_sub)\n",
        "    \n",
        "    print(\"=======================================\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: Logistic Regression\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            " [[85290     9]\n",
            " [   49    95]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85299\n",
            "           1       0.91      0.66      0.77       144\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.96      0.83      0.88     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    99827\n",
            "1      172\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: Decision Tree\n",
            "Confusion matrix:\n",
            " [[85238    61]\n",
            " [   35   109]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85299\n",
            "           1       0.64      0.76      0.69       144\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.82      0.88      0.85     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    99827\n",
            "1      172\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: Random Forest\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            " [[85291     8]\n",
            " [   44   100]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85299\n",
            "           1       0.93      0.69      0.79       144\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.96      0.85      0.90     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    99827\n",
            "1      172\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: SVM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            " [[85293     6]\n",
            " [   59    85]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85299\n",
            "           1       0.93      0.59      0.72       144\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.97      0.80      0.86     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    99827\n",
            "1      172\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTPltQJleNYK",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate with *Undersampled* dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg-QqTekeNYM",
        "colab_type": "code",
        "outputId": "2cd00ef5-06a3-42b6-cbdb-097713a6c5ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Now we will test on Undersampled dataset (X_train_undersample, y_train_undersample)\n",
        "# We loop for models\n",
        "# For each model, we train with train_undersample dataset\n",
        "# and use evaluate_model function to test with test set\n",
        "for idx, model in enumerate(models):\n",
        "    print(\"Model: {}\".format(models_name[idx]))\n",
        "    evaluate_model(model,X_train_undersample,y_train_undersample)\n",
        "    \n",
        "    print(\"=======================================\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: Logistic Regression\n",
            "Confusion matrix:\n",
            " [[82354  2945]\n",
            " [   12   132]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98     85299\n",
            "           1       0.04      0.92      0.08       144\n",
            "\n",
            "    accuracy                           0.97     85443\n",
            "   macro avg       0.52      0.94      0.53     85443\n",
            "weighted avg       1.00      0.97      0.98     85443\n",
            "\n",
            "Testing set information:\n",
            " 1    348\n",
            "0    348\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: Decision Tree\n",
            "Confusion matrix:\n",
            " [[75508  9791]\n",
            " [   12   132]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.89      0.94     85299\n",
            "           1       0.01      0.92      0.03       144\n",
            "\n",
            "    accuracy                           0.89     85443\n",
            "   macro avg       0.51      0.90      0.48     85443\n",
            "weighted avg       1.00      0.89      0.94     85443\n",
            "\n",
            "Testing set information:\n",
            " 1    348\n",
            "0    348\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: Random Forest\n",
            "Confusion matrix:\n",
            " [[82508  2791]\n",
            " [   16   128]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98     85299\n",
            "           1       0.04      0.89      0.08       144\n",
            "\n",
            "    accuracy                           0.97     85443\n",
            "   macro avg       0.52      0.93      0.53     85443\n",
            "weighted avg       1.00      0.97      0.98     85443\n",
            "\n",
            "Testing set information:\n",
            " 1    348\n",
            "0    348\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: SVM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            " [[80779  4520]\n",
            " [   13   131]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97     85299\n",
            "           1       0.03      0.91      0.05       144\n",
            "\n",
            "    accuracy                           0.95     85443\n",
            "   macro avg       0.51      0.93      0.51     85443\n",
            "weighted avg       1.00      0.95      0.97     85443\n",
            "\n",
            "Testing set information:\n",
            " 1    348\n",
            "0    348\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1Sd387SeNYR",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate with *Oversampled* dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovw4FCTXeNYS",
        "colab_type": "code",
        "outputId": "63f58d17-ae2a-4182-de21-dde226672a95",
        "colab": {}
      },
      "source": [
        "# Now we will test on Oversampled dataset (X_train_oversample, y_train_oversample)\n",
        "# We loop for models\n",
        "# For each model, we train with train_oversample dataset\n",
        "# and use evaluate_model function to test with test set\n",
        "for idx, model in enumerate(models):\n",
        "    print(\"Model: {}\".format(models_name[idx]))\n",
        "    # Your code here\n",
        "    print(\"=======================================\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: Logistic Regression\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/dks/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            " [[83392  1903]\n",
            " [   17   131]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     85295\n",
            "           1       0.06      0.89      0.12       148\n",
            "\n",
            "   micro avg       0.98      0.98      0.98     85443\n",
            "   macro avg       0.53      0.93      0.55     85443\n",
            "weighted avg       1.00      0.98      0.99     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    85295\n",
            "1      148\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: Decision Tree\n",
            "Confusion matrix:\n",
            " [[85263    32]\n",
            " [   47   101]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85295\n",
            "           1       0.76      0.68      0.72       148\n",
            "\n",
            "   micro avg       1.00      1.00      1.00     85443\n",
            "   macro avg       0.88      0.84      0.86     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    85295\n",
            "1      148\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: Random Forest\n",
            "Confusion matrix:\n",
            " [[85289     6]\n",
            " [   39   109]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85295\n",
            "           1       0.95      0.74      0.83       148\n",
            "\n",
            "   micro avg       1.00      1.00      1.00     85443\n",
            "   macro avg       0.97      0.87      0.91     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    85295\n",
            "1      148\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: Bernoulli NB\n",
            "Confusion matrix:\n",
            " [[83125  2170]\n",
            " [   26   122]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.99     85295\n",
            "           1       0.05      0.82      0.10       148\n",
            "\n",
            "   micro avg       0.97      0.97      0.97     85443\n",
            "   macro avg       0.53      0.90      0.54     85443\n",
            "weighted avg       1.00      0.97      0.99     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    85295\n",
            "1      148\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n",
            "Model: Gaussian NB\n",
            "Confusion matrix:\n",
            " [[84654   641]\n",
            " [   31   117]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     85295\n",
            "           1       0.15      0.79      0.26       148\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     85443\n",
            "   macro avg       0.58      0.89      0.63     85443\n",
            "weighted avg       1.00      0.99      0.99     85443\n",
            "\n",
            "Testing set information:\n",
            " 0    85295\n",
            "1      148\n",
            "Name: Class, dtype: int64\n",
            "=======================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krkbxIGseNYV",
        "colab_type": "text"
      },
      "source": [
        "## 5. GridsearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcxEFusNeNYW",
        "colab_type": "text"
      },
      "source": [
        "- pipline: http://queirozf.com/entries/scikit-learn-pipeline-examples\n",
        "- GridsearchCV: https://chrisalbon.com/machine_learning/model_evaluation/cross_validation_parameter_tuning_grid_search/\n",
        "- GridsearchCV scorer:https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJgGEH8ueNYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Pipeline and GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJu20Lb3eNYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "step = []\n",
        "# append a step 'cls' with value is LogisticRegression to step variable\n",
        "# Your code here\n",
        "# Create Pipeline with defined step\n",
        "ppl = Pipeline(step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZrR7GPoeNYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define params grid, gridsearch cv go throuh each item in params_grid\n",
        "# For each item, it changes param of Pipeline base on \"key\" of item\n",
        "params_grid = {\n",
        "    \"cls\": models\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA8D56rjeNYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
        "# Define metrics to evaluate model\n",
        "scorers = {\n",
        "    'recall_score': make_scorer(recall_score),\n",
        "    'precision_score': make_scorer(precision_score),\n",
        "    'accuracy_score': make_scorer(accuracy_score)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWpPXtR-eNYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create GridSearchCV with Pipeline as estimator and params_grid\n",
        "gridcv = GridSearchCV(ppl, param_grid=params_grid, scoring=scorers, refit='recall_score', return_train_score=True, verbose=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKVnBYo0eNYv",
        "colab_type": "code",
        "outputId": "cd2a3a4f-1d5f-469d-e02e-be3132271194",
        "colab": {}
      },
      "source": [
        "# train model as a normal model with fit\n",
        "gridcv.fit(X_train_oversample, y_train_oversample)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/dks/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/home/dks/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "[CV] cls=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
            "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
            "          tol=0.0001, verbose=0, warm_start=False) \n",
            "[CV]  cls=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
            "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
            "          tol=0.0001, verbose=0, warm_start=False), recall_score=0.9403491901313219, precision_score=0.9789457218833099, accuracy_score=0.9600625056345223, total=   1.7s\n",
            "[CV] cls=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
            "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
            "          tol=0.0001, verbose=0, warm_start=False) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n",
            "/home/dks/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  cls=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
            "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
            "          tol=0.0001, verbose=0, warm_start=False), recall_score=0.9426029990684256, precision_score=0.9770129263354618, accuracy_score=0.9602127595636626, total=   1.6s\n",
            "[CV] cls=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
            "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
            "          tol=0.0001, verbose=0, warm_start=False) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.4s remaining:    0.0s\n",
            "/home/dks/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  cls=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
            "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
            "          tol=0.0001, verbose=0, warm_start=False), recall_score=0.9412489481908883, precision_score=0.9769189981597579, accuracy_score=0.959505349200625, total=   1.7s\n",
            "[CV] cls=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "            max_features=None, max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
            "            splitter='best') \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  cls=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "            max_features=None, max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
            "            splitter='best'), recall_score=1.0, precision_score=0.9995794659217205, accuracy_score=0.9997896444992036, total=   2.2s\n",
            "[CV] cls=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "            max_features=None, max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
            "            splitter='best') \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    7.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  cls=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "            max_features=None, max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
            "            splitter='best'), recall_score=1.0, precision_score=0.999639520562348, accuracy_score=0.9998196952850317, total=   2.1s\n",
            "[CV] cls=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "            max_features=None, max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
            "            splitter='best') \n",
            "[CV]  cls=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "            max_features=None, max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
            "            splitter='best'), recall_score=1.0, precision_score=0.9994593620472157, accuracy_score=0.9997295347998557, total=   2.2s\n",
            "[CV] cls=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
            "            oob_score=False, random_state=None, verbose=0,\n",
            "            warm_start=False) \n",
            "[CV]  cls=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
            "            oob_score=False, random_state=None, verbose=0,\n",
            "            warm_start=False), recall_score=1.0, precision_score=0.9998798113037469, accuracy_score=0.9999398984283439, total=   2.8s\n",
            "[CV] cls=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
            "            oob_score=False, random_state=None, verbose=0,\n",
            "            warm_start=False) \n",
            "[CV]  cls=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
            "            oob_score=False, random_state=None, verbose=0,\n",
            "            warm_start=False), recall_score=1.0, precision_score=0.9998798113037469, accuracy_score=0.9999398984283439, total=   2.7s\n",
            "[CV] cls=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
            "            oob_score=False, random_state=None, verbose=0,\n",
            "            warm_start=False) \n",
            "[CV]  cls=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "            min_samples_leaf=1, min_samples_split=2,\n",
            "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
            "            oob_score=False, random_state=None, verbose=0,\n",
            "            warm_start=False), recall_score=1.0, precision_score=0.9999699492141719, accuracy_score=0.9999849741555475, total=   2.5s\n",
            "[CV] cls=GaussianNB(priors=None, var_smoothing=1e-09) ................\n",
            "[CV]  cls=GaussianNB(priors=None, var_smoothing=1e-09), recall_score=0.8589716621089641, precision_score=0.9731717281764947, accuracy_score=0.9176458214382306, total=   0.1s\n",
            "[CV] cls=GaussianNB(priors=None, var_smoothing=1e-09) ................\n",
            "[CV]  cls=GaussianNB(priors=None, var_smoothing=1e-09), recall_score=0.8609850647594435, precision_score=0.9708254269449715, accuracy_score=0.9175556690807465, total=   0.1s\n",
            "[CV] cls=GaussianNB(priors=None, var_smoothing=1e-09) ................\n",
            "[CV]  cls=GaussianNB(priors=None, var_smoothing=1e-09), recall_score=0.8577653564130304, precision_score=0.9718089271730619, accuracy_score=0.9164412789998798, total=   0.1s\n",
            "[CV] cls=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) \n",
            "[CV]  cls=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True), recall_score=0.8085163927036692, precision_score=0.9914142530768664, accuracy_score=0.9007572798028668, total=   0.2s\n",
            "[CV] cls=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) \n",
            "[CV]  cls=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True), recall_score=0.7987198365237251, precision_score=0.991309861256154, accuracy_score=0.8958590017128948, total=   0.2s\n",
            "[CV] cls=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) \n",
            "[CV]  cls=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True), recall_score=0.8027407140281284, precision_score=0.9909114515710206, accuracy_score=0.8976890251232119, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   22.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
              "       estimator=Pipeline(memory=None,\n",
              "     steps=[('cls', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False))]),\n",
              "       fit_params=None, iid='warn', n_jobs=None,\n",
              "       param_grid={'cls': [LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False), DecisionTreeCla...None, var_smoothing=1e-09), BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)]},\n",
              "       pre_dispatch='2*n_jobs', refit='recall_score',\n",
              "       return_train_score=True,\n",
              "       scoring={'recall_score': make_scorer(recall_score), 'precision_score': make_scorer(precision_score), 'accuracy_score': make_scorer(accuracy_score)},\n",
              "       verbose=5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWd8elrmeNYy",
        "colab_type": "code",
        "outputId": "71413809-46c4-46c8-e7b9-15a9e6d683a9",
        "colab": {}
      },
      "source": [
        "results = pd.DataFrame(gridcv.cv_results_)\n",
        "results = results.sort_values(by='mean_test_precision_score', ascending=False)\n",
        "results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_cls']].round(3).head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_test_precision_score</th>\n",
              "      <th>mean_test_recall_score</th>\n",
              "      <th>mean_test_accuracy_score</th>\n",
              "      <th>param_cls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.991</td>\n",
              "      <td>0.803</td>\n",
              "      <td>0.898</td>\n",
              "      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.978</td>\n",
              "      <td>0.941</td>\n",
              "      <td>0.960</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.972</td>\n",
              "      <td>0.859</td>\n",
              "      <td>0.917</td>\n",
              "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_test_precision_score  mean_test_recall_score  \\\n",
              "2                      1.000                   1.000   \n",
              "1                      1.000                   1.000   \n",
              "4                      0.991                   0.803   \n",
              "0                      0.978                   0.941   \n",
              "3                      0.972                   0.859   \n",
              "\n",
              "   mean_test_accuracy_score                                          param_cls  \n",
              "2                     1.000  (DecisionTreeClassifier(class_weight=None, cri...  \n",
              "1                     1.000  DecisionTreeClassifier(class_weight=None, crit...  \n",
              "4                     0.898  BernoulliNB(alpha=1.0, binarize=0.0, class_pri...  \n",
              "0                     0.960  LogisticRegression(C=1.0, class_weight=None, d...  \n",
              "3                     0.917       GaussianNB(priors=None, var_smoothing=1e-09)  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFQLLlkeeNY1",
        "colab_type": "code",
        "outputId": "f55f5651-1e7f-410c-9b7e-0b8dc5b75271",
        "colab": {}
      },
      "source": [
        "results.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_cls</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_recall_score</th>\n",
              "      <th>split1_test_recall_score</th>\n",
              "      <th>split2_test_recall_score</th>\n",
              "      <th>mean_test_recall_score</th>\n",
              "      <th>...</th>\n",
              "      <th>split1_test_accuracy_score</th>\n",
              "      <th>split2_test_accuracy_score</th>\n",
              "      <th>mean_test_accuracy_score</th>\n",
              "      <th>std_test_accuracy_score</th>\n",
              "      <th>rank_test_accuracy_score</th>\n",
              "      <th>split0_train_accuracy_score</th>\n",
              "      <th>split1_train_accuracy_score</th>\n",
              "      <th>split2_train_accuracy_score</th>\n",
              "      <th>mean_train_accuracy_score</th>\n",
              "      <th>std_train_accuracy_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.521150</td>\n",
              "      <td>0.154356</td>\n",
              "      <td>0.156230</td>\n",
              "      <td>0.004852</td>\n",
              "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
              "      <td>{'cls': (DecisionTreeClassifier(class_weight=N...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.999940</td>\n",
              "      <td>0.999985</td>\n",
              "      <td>0.999955</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.160425</td>\n",
              "      <td>0.036160</td>\n",
              "      <td>0.032889</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
              "      <td>{'cls': DecisionTreeClassifier(class_weight=No...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.999820</td>\n",
              "      <td>0.999730</td>\n",
              "      <td>0.999780</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.074541</td>\n",
              "      <td>0.005205</td>\n",
              "      <td>0.097049</td>\n",
              "      <td>0.000851</td>\n",
              "      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n",
              "      <td>{'cls': BernoulliNB(alpha=1.0, binarize=0.0, c...</td>\n",
              "      <td>0.808516</td>\n",
              "      <td>0.798720</td>\n",
              "      <td>0.802741</td>\n",
              "      <td>0.803326</td>\n",
              "      <td>...</td>\n",
              "      <td>0.895859</td>\n",
              "      <td>0.897689</td>\n",
              "      <td>0.898102</td>\n",
              "      <td>0.002021</td>\n",
              "      <td>5</td>\n",
              "      <td>0.898442</td>\n",
              "      <td>0.896293</td>\n",
              "      <td>0.900074</td>\n",
              "      <td>0.898270</td>\n",
              "      <td>0.001548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.631225</td>\n",
              "      <td>0.063980</td>\n",
              "      <td>0.027371</td>\n",
              "      <td>0.006508</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>{'cls': LogisticRegression(C=1.0, class_weight...</td>\n",
              "      <td>0.940349</td>\n",
              "      <td>0.942603</td>\n",
              "      <td>0.941249</td>\n",
              "      <td>0.941400</td>\n",
              "      <td>...</td>\n",
              "      <td>0.960213</td>\n",
              "      <td>0.959505</td>\n",
              "      <td>0.959927</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>3</td>\n",
              "      <td>0.960069</td>\n",
              "      <td>0.959461</td>\n",
              "      <td>0.960325</td>\n",
              "      <td>0.959952</td>\n",
              "      <td>0.000363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.055097</td>\n",
              "      <td>0.000861</td>\n",
              "      <td>0.065142</td>\n",
              "      <td>0.003063</td>\n",
              "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
              "      <td>{'cls': GaussianNB(priors=None, var_smoothing=...</td>\n",
              "      <td>0.858972</td>\n",
              "      <td>0.860985</td>\n",
              "      <td>0.857765</td>\n",
              "      <td>0.859241</td>\n",
              "      <td>...</td>\n",
              "      <td>0.917556</td>\n",
              "      <td>0.916441</td>\n",
              "      <td>0.917214</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>4</td>\n",
              "      <td>0.917201</td>\n",
              "      <td>0.916968</td>\n",
              "      <td>0.917353</td>\n",
              "      <td>0.917174</td>\n",
              "      <td>0.000158</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  39 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
              "2       2.521150      0.154356         0.156230        0.004852   \n",
              "1       2.160425      0.036160         0.032889        0.001253   \n",
              "4       0.074541      0.005205         0.097049        0.000851   \n",
              "0       1.631225      0.063980         0.027371        0.006508   \n",
              "3       0.055097      0.000861         0.065142        0.003063   \n",
              "\n",
              "                                           param_cls  \\\n",
              "2  (DecisionTreeClassifier(class_weight=None, cri...   \n",
              "1  DecisionTreeClassifier(class_weight=None, crit...   \n",
              "4  BernoulliNB(alpha=1.0, binarize=0.0, class_pri...   \n",
              "0  LogisticRegression(C=1.0, class_weight=None, d...   \n",
              "3       GaussianNB(priors=None, var_smoothing=1e-09)   \n",
              "\n",
              "                                              params  \\\n",
              "2  {'cls': (DecisionTreeClassifier(class_weight=N...   \n",
              "1  {'cls': DecisionTreeClassifier(class_weight=No...   \n",
              "4  {'cls': BernoulliNB(alpha=1.0, binarize=0.0, c...   \n",
              "0  {'cls': LogisticRegression(C=1.0, class_weight...   \n",
              "3  {'cls': GaussianNB(priors=None, var_smoothing=...   \n",
              "\n",
              "   split0_test_recall_score  split1_test_recall_score  \\\n",
              "2                  1.000000                  1.000000   \n",
              "1                  1.000000                  1.000000   \n",
              "4                  0.808516                  0.798720   \n",
              "0                  0.940349                  0.942603   \n",
              "3                  0.858972                  0.860985   \n",
              "\n",
              "   split2_test_recall_score  mean_test_recall_score            ...             \\\n",
              "2                  1.000000                1.000000            ...              \n",
              "1                  1.000000                1.000000            ...              \n",
              "4                  0.802741                0.803326            ...              \n",
              "0                  0.941249                0.941400            ...              \n",
              "3                  0.857765                0.859241            ...              \n",
              "\n",
              "   split1_test_accuracy_score  split2_test_accuracy_score  \\\n",
              "2                    0.999940                    0.999985   \n",
              "1                    0.999820                    0.999730   \n",
              "4                    0.895859                    0.897689   \n",
              "0                    0.960213                    0.959505   \n",
              "3                    0.917556                    0.916441   \n",
              "\n",
              "   mean_test_accuracy_score  std_test_accuracy_score  \\\n",
              "2                  0.999955                 0.000021   \n",
              "1                  0.999780                 0.000037   \n",
              "4                  0.898102                 0.002021   \n",
              "0                  0.959927                 0.000304   \n",
              "3                  0.917214                 0.000548   \n",
              "\n",
              "   rank_test_accuracy_score  split0_train_accuracy_score  \\\n",
              "2                         1                     1.000000   \n",
              "1                         2                     1.000000   \n",
              "4                         5                     0.898442   \n",
              "0                         3                     0.960069   \n",
              "3                         4                     0.917201   \n",
              "\n",
              "   split1_train_accuracy_score  split2_train_accuracy_score  \\\n",
              "2                     1.000000                     1.000000   \n",
              "1                     1.000000                     1.000000   \n",
              "4                     0.896293                     0.900074   \n",
              "0                     0.959461                     0.960325   \n",
              "3                     0.916968                     0.917353   \n",
              "\n",
              "   mean_train_accuracy_score  std_train_accuracy_score  \n",
              "2                   1.000000                  0.000000  \n",
              "1                   1.000000                  0.000000  \n",
              "4                   0.898270                  0.001548  \n",
              "0                   0.959952                  0.000363  \n",
              "3                   0.917174                  0.000158  \n",
              "\n",
              "[5 rows x 39 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zWNY_MaeNY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}